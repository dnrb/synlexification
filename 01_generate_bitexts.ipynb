{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c5eed1-c125-4141-abcf-a67a627462a5",
   "metadata": {},
   "source": [
    "## Imports and globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "106e5d05-2e00-448b-a0db-38179b6480d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fb41c8-c3a6-46bd-9fac-e19a6b7dbfdf",
   "metadata": {},
   "source": [
    "## Bible preprocessing code\n",
    "See ENGWEB.csv for an example of a corpus file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f48d9cbc-b239-420b-89a6-a9bab9189187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_word(w, do_unidecode=False):\n",
    "    \"\"\"\n",
    "    input is a word (string) w\n",
    "    the function strips a word of punctuation (loaded from the morphology module)\n",
    "    and depending do_unicode, further replaces non-unicode characters by unicode characters\n",
    "    returns the resulting string\n",
    "    \"\"\"\n",
    "    if do_unidecode: \n",
    "        return re.sub(' ', '', unidecode(w.lower(), errors='preserve').strip(punct).lower())\n",
    "    else: \n",
    "        return re.sub(' ', '', w.strip(punct).lower())\n",
    "\n",
    "def clean_sent(s):\n",
    "    \"\"\"\n",
    "    takes a sentence s (a list of strings) and adds start-of-string and end-of-string $ characters to\n",
    "    the cleaned words (string)\n",
    "    \"\"\"\n",
    "    return list(filter(lambda w : w != '$$', \n",
    "                       map(lambda w : '$' + clean_word(w) + '$', s.split())))\n",
    "\n",
    "def read_in_bible(path):\n",
    "    \"\"\"\n",
    "    reads in a Bible translation from the csv into a dictionary with keys (triples of book, chapter, verse)\n",
    "    mapped to values (cleaned sentences)\n",
    "    \"\"\"\n",
    "    tb = pd.read_csv(path)\n",
    "    tbd = {(t.book,t.chapter,t.verse):clean_sent(t.text) for i,t in tb.iterrows() if isinstance(t.text, str)}\n",
    "    tbd = {k:v for k,v in tbd.items() if len(v) > 0}\n",
    "    return tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "814435a8-d2d3-4ad2-b927-a480ddce1ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_bitext(l, sbd):\n",
    "    tbd = read_in_bible(bib_dir + '%s.csv' % l)\n",
    "    with open('./bitexts/%s.txt' % l, 'w') as fout:\n",
    "        for v in sbd:\n",
    "            try: ts = ' '.join(tbd[v])\n",
    "            except: ts = ''\n",
    "            fout.write(ts + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a892bef7-107f-40da-bde3-0ff2c4553f12",
   "metadata": {},
   "source": [
    "## CELEX functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cfcdbc40-8138-42d7-8b74-c2404b85321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_celex():\n",
    "    emw = [l.strip('\\n').split('\\\\') for l in open(emw_path)]\n",
    "    eml = [l.strip('\\n').split('\\\\') for l in open(eml_path)]\n",
    "    #\n",
    "    w_map = {e[1]:e[3] for e in emw}\n",
    "    l_map = {e[0]: re.findall(r'\\(\\w+\\)\\[.*?\\]', e[21]) for e in eml if e[21] != ''}\n",
    "    return w_map, l_map   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "24791b05-fc8b-48ea-8be8-ccb07fdd2912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_celex_parse(p):\n",
    "    labs = {'N' : 'NOUN', 'V' : 'VERB', 'P' : 'ADP', 'A' : 'ADJ', 'B' : 'ADV', 'Q' : 'NUM', \n",
    "            'O' : 'PRON', 'D' : 'DET', 'X' : 'AFX', 'C' : 'SCONJ'}\n",
    "    buf = ''\n",
    "    bufcat = None\n",
    "    done = []\n",
    "    for pi in p:\n",
    "        pw,pc = re.sub(r'\\((\\w+)\\).*', r'\\1', pi), re.sub(r'.*\\[([^\\]]+)\\]', r'\\1', pi)\n",
    "        if 'x' in pc:\n",
    "            buf += ('_' if buf != '' else '') + pw\n",
    "            bufcat = pc.split('|')[0] if set(pc.split('|')[1]) == {'.', 'x'} else 'X'\n",
    "        else:\n",
    "            if '|' in pc:\n",
    "                pc = 'X'\n",
    "            done.append(pw + '/' + labs[pc])\n",
    "    \n",
    "    if buf != '': \n",
    "        done.append(buf + '/' + labs[bufcat])\n",
    "    return done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc69887-ec15-4d70-a42b-67cc3d7589d1",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76010cfe-f719-4d3e-a9f9-5ec515066dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "punct = '.?!,\"\\':;()[]‘’“”–-«»\\t\\n\\r!\"#$%&\\'()*+,-./:;<=>?[]^_`{|}~\\¡£§©ª«¬´¶·»¿×ʻʼˆˇˈˉˊˋˍː˜ˬ˻˼̵̶̸̧̨̠̣̤̥̦̬̭̯̰̱̲̀́̂̃̄̆̇̈̌̍̓͏͘;·،؛؟‐‑–—―‘’‚‛“”„‟•…‸‹›⅓⅟ↄ→−≪≫⌊⌋⌞⌟、。〉「」『』【】〔〕ꓸꓹꓺꓻꓼꓽ꓾꓿꞉꞊ꞌ﹐﹕﹖﹗！（），－．：；＜＞？'\n",
    "bib_dir = '../../../../data/bibles_2021/' # replace by path to directory where bible files live\n",
    "emw_path = './emw.cd' # path to CELEX file ewm.cd\n",
    "eml_path = './eml.cd' # path to CELEX file eml.cd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ebbac-6d61-4462-ba20-480abc913fdc",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40a814ec-0ace-4477-861f-2590a0a979f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "good_books = {'JAS', '2JN', '1JN', 'EPH', '2TH', 'JHN', 'COL', 'REV', 'GAL', 'ACT', 'MRK', 'JUD', '1TH', '1PE', '3JN', 'LUK', \n",
    "              'ROM', '2PE', '2CO', 'PHP', 'TIT', '1TI', 'PHM', '2TI', '1CO', 'HEB', 'MAT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "868359a0-f664-4609-a708-10c749fb9354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in seed bible\n",
    "sbd_full_raw = {(r.book, r.chapter, r.verse) : r.text for i,r in pd.read_csv(bib_dir + 'ENGWEB.csv').iterrows()\n",
    "                if r.book in good_books and isinstance(r.text, str)}\n",
    "sbd_full_spc = {k : nlp(v) for k,v in sbd_full_raw.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c3e79c2-eee6-41be-b1ad-c007321ad0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# celex\n",
    "wmap, lmap = read_celex()\n",
    "swords = Counter([w.lemma_ for l in sbd_full_spc.values() for w in l])\n",
    "substitutions = {}\n",
    "for w in swords:\n",
    "    ww = w.split('/')[0]\n",
    "    if ww in wmap and wmap[ww] in lmap and len(lmap[wmap[ww]]) > 1:\n",
    "        substitutions[w] = transform_celex_parse(lmap[wmap[ww]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d6bd3aea-35b2-49a3-87de-1ffa8df8fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write out seed doculect files\n",
    "if not os.path.isdir('./bitexts/'): \n",
    "    os.mkdir('./bitexts')\n",
    "    \n",
    "with open('./bitexts/seed.txt', 'w') as fout:\n",
    "    for v,l in sbd_full_spc.items():\n",
    "        rep_line = chain(*map(lambda w : (substitutions[w.lemma_] if w.lemma_ in substitutions else [w.lemma_+'/'+w.pos_]),\n",
    "                              filter(lambda w : w.pos_ != 'PUNCT', l)))\n",
    "        fout.write(' '.join(rep_line) + '\\n')\n",
    "\n",
    "with open('./bitexts/seed.dep', 'w') as fout:\n",
    "    for v,l in sbd_full_spc.items():\n",
    "        rep_line = chain(*map(lambda w : (map(lambda x : '%s/%s/%s' % (x, w.i, w.head.i), substitutions[w.lemma_]) \n",
    "                                          if w.lemma_ in substitutions \n",
    "                                          else ['/'.join(map(str,(w.lemma_,w.pos_,w.i,w.head.i)))]), \n",
    "                              filter(lambda w : w.pos_ != 'PUNCT', l)))\n",
    "        fout.write(' '.join(rep_line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "eb6079d4-6de7-45fc-a3bb-6710699a6c24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAUWBT ACMAS3 ACUTBL AGGPNG AGMWBT ALYXXX AMFSIM AMKWBT AMMWBT AMNPNG AMPWBT AMRTBL AMUMVR AOJFIL ARLTBL AVAANT AVTWBT AZZTBL BBOBSM BDHWBT BFDWBT BIBWBT BKLLAI BOATBL BORWYI BRUNXB BSCWBT BVRXXX BVZYSS BYRWBT BYXWBT CABNVS CAPSBB CASNTM CAXSBB CBITBL CBTTBL CCOTBL CHEIBT CHRPDV CJPTJV CMEWBT CONWBT CRHIBT CRKWCV CRNWBT CRXWYI CSKATB CTGBSB DESWBT DIDWBT DIFXXX DJKWBT DTSABM DUDWYI ELLELL ESEE06 ESSWYI EUSNLT FRDWBT FUVLTBL GAHPNG GBILAI GHSPNG GRTBBS GUCTBL GUHWBT GUKBSE GUPXXX HADLAI HAKTHV HTOWBT HUNK90 HUVTBL HWCWYI IANPNG IBATIV IFBTBL INDASV IRKBST ITAR27 IZZTBL JAMBSW JAVNRF JBUIBS JICWBT KABCEB KBHWBT KERABT KFBNTA KGRLAI KHGNTV KHQBIV KIAWBT KMOWBT KMSPNG KNJSBI KORSYS KPVIBT KPWPNG KRLNEW KRSWYI KTOWBT KYCPNG LEFTBL LMEABT MAKLAI MBCWBT MCAWBT MDYBSE MEJTBL MFEBSM MFYWBT MHIBSU MHRIBT MIFWBT MILTBL MIQSBN MLPTBL MOPWBT MORBSS MPMTBL MPTWBT MSYPNG MTOTBL MWWHDV MZMWBT NABWBT NAFWBT NASPNG NHXNFB NIAIBS NIJLAI NLDHSV NOAWBT NTJXXX NTPTBL NUYXXX OPMTBL OTQTBL PADWBT PAMPBS PAONAB PAUPAL PBBDYU PJTXXX POEWBT POIWBT PPOWBT PRKBSM PUIABC QUBPBS RAWBIB RELBTL ROOWBT SABWBT SGWBSE SHKBSS SPPTBL SRNBSS SSDWBT SURIBS SXNLAI TABIBT TACPBC TBGWBT TCATBL TCCBST TCSWYI TEETBL TEOBSU TFRWBT THATSV TIHBSM TIKWYI TLJWBT TOPTBL TPIPNG TPTTBL TQOTQO TRCWBT TUFWYI URATBL URBWBT VIELHG WBABIV WIMWYI XALIBT XAVTBL XSUMEV YADTBL YLEWBT YSSYYV YUJWBT YUZNTM YVATBL ZNEZNE ZPMTBL "
     ]
    }
   ],
   "source": [
    "# write out target doculects\n",
    "doculects = pd.read_csv('./files/language_sample.csv',sep='\\t').language.to_list()\n",
    "#\n",
    "for doc in sorted(doculects):\n",
    "    print(doc, end = ' ')\n",
    "    write_bitext(doc, sbd_full_spc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24bd08e-0ea6-4404-b212-34302124e67f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create morphologically segmented target language files\n",
    "for targtype in ['mrph','mrax']:\n",
    "    for doc in sorted(doculects):\n",
    "        print(targtype, doc)\n",
    "        morph = dict(map(lambda x : (x.split('\\t')[0], x.strip('\\n').split('\\t')[1].split()), open('./vorm/%s.lab' % doc)))\n",
    "        with open('./bitexts/%s.%s' % (doc,targtype),'w') as fout:\n",
    "            for l1 in open('./bitexts/%s.txt' % doc):\n",
    "                morphemes = chain(*map(lambda x : morph[x.strip('$')], \n",
    "                                       l1.strip('\\n').split()))\n",
    "                if targtype == 'mrph': \n",
    "                    stc = ' '.join(filter(lambda m : m.split('/')[1] == 'STM', morphemes))\n",
    "                elif targtype == 'mrax': \n",
    "                    stc = ' '.join(morphemes) \n",
    "                fout.write(stc + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
