{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201aecf9-5c09-4204-ac15-b7b8c404c8eb",
   "metadata": {},
   "source": [
    "## preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1158f492-ec5a-4225-8927-9ccadbd6dc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from scipy.stats import fisher_exact\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from datetime import datetime\n",
    "import os\n",
    "from multiprocessing import Pool\n",
    "from subprocess import call\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc1f0e0-e58d-47b7-a238-55a15187df5b",
   "metadata": {},
   "source": [
    "## Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "250b52d6-8262-4047-bf88-92cce89fe020",
   "metadata": {},
   "outputs": [],
   "source": [
    "doculects = pd.read_csv('./files/language_sample.csv', sep = '\\t').language.to_list()\n",
    "#\n",
    "MIN_P = 1e-6 # theta_fe in the paper\n",
    "MIN_D = 3 # theta_bt in the paper\n",
    "#\n",
    "core_pos = {'NOUN','ADJ','VERB'}\n",
    "peri_pos = {'ADP', 'ADV', 'PART', 'PROPN', 'AFX'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77888a67-f814-41b4-9339-b0bdb3e30cd9",
   "metadata": {},
   "source": [
    "## Run eflomal, Round 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "690a5892-17a3-42d7-a68c-9ad0ca658239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_eflomal(doc, seedtype, targtype='mrph', outdir='./alignments/'):\n",
    "    #\n",
    "    fwdname = outdir + doc + (('.' + seedtype) if seedtype != 'txt' else '') + '.fwd'\n",
    "    revname = outdir + doc + (('.' + seedtype) if seedtype != 'txt' else '') + '.rev'\n",
    "    symname = outdir + doc + (('.' + seedtype) if seedtype != 'txt' else '') + '.sym'\n",
    "    if not os.path.isfile(fwdname):\n",
    "        call(['eflomal-align', '-s', './bitexts/seed.%s' % seedtype, '-t', './bitexts/%s.%s' % (doc, targtype), '--model', '3', \n",
    "              '-f', fwdname, '-r', revname])\n",
    "    with open(symname, 'w') as fout:\n",
    "        call(['./atools', '-c', 'grow-diag-final-and', '-i', fwdname, '-j', revname], stdout=fout)\n",
    "    print(doc, datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db37bda7-f1dc-43c5-8c4e-3850c79bc734",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with Pool(10) as p:\n",
    "    p.starmap(run_eflomal, map(lambda doc : (doc, 'txt', 'mrph', './alignments/'), doculects))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8d8fc2-5161-4805-9ee3-39c73c89ca0c",
   "metadata": {},
   "source": [
    "## Step 1: create a circumlexified corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343c002e-2637-42ac-8de4-2a1653e3cd08",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a27bfc23-9e67-4c8d-b30c-9820d0d34cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seed_index(sbd, core_words):\n",
    "    \"\"\"\n",
    "    mapping all tokens of core words (N,A,V), represented as line-ix, token-ix pairs to a unique index\n",
    "    \"\"\"\n",
    "    seed_index = {}\n",
    "    for vi,l in enumerate(sbd):\n",
    "        for wi,w in enumerate(l):\n",
    "            if w.split('/')[0] in core_words:\n",
    "                seed_index[vi,wi] = len(seed_index)\n",
    "    print(len(seed_index))\n",
    "    return seed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78f6d04c-8851-45e1-a67c-ddfbdc45ebb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_counts_and_builder(doculects, core_words):\n",
    "    \"\"\"\n",
    "    gather alignment counts and uniquely aligned words in target languages given a seed language word\n",
    "    and a 'builder' object containing all relevant seed languages mapped to their translations in F\n",
    "    \"\"\"\n",
    "    f2e = defaultdict(lambda : defaultdict(lambda : Counter()))\n",
    "    builder = defaultdict(lambda : set())\n",
    "    #\n",
    "    for di,doc in enumerate(doculects):\n",
    "        if di % 25 == 0: print(di,doc,datetime.now())\n",
    "        for i,(a,l1,l2) in enumerate(zip(open('./alignments/' + doc + '.sym'),\n",
    "                                         open('./bitexts/seed.txt'),\n",
    "                                         open('./bitexts/%s.mrph'% doc))):\n",
    "            a = [tuple(map(int, ax.split('-'))) for ax in a.strip('\\n').split()]\n",
    "            w1,w2 = l1.strip('\\n').split(), l2.strip('\\n').split()\n",
    "            f2e_a,e2f_a = defaultdict(lambda : set()),defaultdict(lambda : set())\n",
    "            for ae,af in a:\n",
    "                f2e_a[af].add(ae)\n",
    "                e2f_a[ae].add(af)\n",
    "            for ae, AF in e2f_a.items():\n",
    "                we = w1[ae]\n",
    "                if we.split('/')[0].lower() in core_words:\n",
    "                    WF = {(doc,w2[af]) for af in AF if f2e_a[af]-{ae} == set()}\n",
    "                    # only translations to AF that do not backtranslate to any other word than AE\n",
    "                    builder[i,ae,we] |= WF\n",
    "            for ae,af in a:\n",
    "                we, wf = w1[ae].split('/')[0], w2[af]\n",
    "                #if we.split('/')[1] in core_pos | peri_pos:\n",
    "                f2e[doc][wf][we] += 1\n",
    "    return f2e, builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7a2bb26-b857-4b1e-aef0-883358be8aba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_tok2bt(builder, bt_max):\n",
    "    \"\"\"\n",
    "    given the translations, retrieve mapping to backtranslations for each target doculect\n",
    "    \"\"\"\n",
    "    tok2bt = {}\n",
    "    lemct = Counter()\n",
    "    for i,(k,v) in enumerate(builder.items()):\n",
    "        if i % 10000 == 0: print(i,len(builder),k, datetime.now())\n",
    "        lemct[k[2].split('/')[0].lower()] += 1\n",
    "        bts = defaultdict(lambda : set())\n",
    "        for h in v:\n",
    "            #print(h)\n",
    "            bt = bt_max[h[0]][h[1]]\n",
    "            if bt != None:\n",
    "                bts[h[0]].add(bt)\n",
    "        tok2bt[k] = {t: bt for t,bt in bts.items()}\n",
    "    return tok2bt, lemct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a34d4ef-70e9-45a7-8353-ea5136941f63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_typ2bt(tok2bt):\n",
    "    \"\"\"\n",
    "    aggregate tok2bt into types\n",
    "    \"\"\"\n",
    "    typ2bt = defaultdict(lambda : defaultdict(lambda : Counter()))\n",
    "    lgtot = Counter()\n",
    "    for k,bts in tok2bt.items():\n",
    "        for lg,bt in bts.items():\n",
    "            #if k[2].split('/')[0].lower() in bt: continue\n",
    "            for i in range(len(bt)):\n",
    "                for bti in combinations(bt,i+1):\n",
    "                    #if k[2].split('/')[0].lower() in bt and k[2].split('/')[0].lower() not in bti: continue\n",
    "                    typ2bt[k[2].split('/')[0].lower()][lg][frozenset(bti)] += 1\n",
    "            lgtot[lg] += 1\n",
    "    return typ2bt, lgtot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1167fe15-6f3d-421f-99c1-eaeb2d190a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lg2bt(typ2bt):\n",
    "    lg2bt = {}#defaultdict(lambda : Counter())\n",
    "    for lem in typ2bt:\n",
    "        for lg in typ2bt[lem]:\n",
    "            if lg not in lg2bt: \n",
    "                lg2bt[lg] = {}\n",
    "            for bt,v in typ2bt[lem][lg].items():\n",
    "                lg2bt[lg][bt] = lg2bt[lg].get(bt, 0) + v\n",
    "    return lg2bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "639d0bfb-216c-4ebd-ab57-8f2734b18116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def calculate_fisher_exact(a,b,c,d):\n",
    "    if a/(a+b) < c/(c+d) or a/(a+c) < b/(b+d): \n",
    "        return 0\n",
    "    return -np.log(fisher_exact([[a,b],[c,d]], alternative='greater')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40064c03-a7eb-4111-bc49-667a0e2bb40a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_candidates(typ2bt, lemct, lg2bt, lgtot, core_words, peri_words):\n",
    "    good_inputs = [(lem, lg, bt, a) for lem in typ2bt for lg in typ2bt[lem] for bt,a in typ2bt[lem][lg].items()\n",
    "                   if (len(bt) > 1 and \n",
    "                       any(map(lambda e : e.split('/')[0] in core_words, bt)) and\n",
    "                       all(map(lambda e : e.split('/')[0] in peri_words, bt)))]\n",
    "    pool_inputs = ((a, lemct[lem]-a, lg2bt[lg][bt]-a, lgtot[lg] - (lemct[lem]+lg2bt[lg][bt]-2*a))\n",
    "                    for lem, lg, bt, a in good_inputs)\n",
    "    with Pool(12) as p:\n",
    "        scores = p.starmap(calculate_fisher_exact, pool_inputs)\n",
    "    candidates = {}\n",
    "    for (lem, lg, bt, a), score in zip(good_inputs, scores):\n",
    "        if score > -np.log(MIN_P):\n",
    "            candidates[lem,bt] = candidates.get((lem,bt), {}) | {lg:score}\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a63f446c-489e-483f-89b1-80876e6b08a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def creates_a_cycle(replace_graph, top):\n",
    "    replace_graph.add_edges_from(((top[0],ki) for ki in top[1] if top[0] != ki))\n",
    "    try:\n",
    "        c = nx.find_cycle(replace_graph)\n",
    "        replace_graph.remove_edges_from(((top[0],ki) for ki in top[1] if top[0] != ki))\n",
    "        return replace_graph, True\n",
    "    except nx.NetworkXNoCycle:\n",
    "        return replace_graph, False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e54de-8302-4913-ba44-3d08d3168f27",
   "metadata": {},
   "source": [
    "### gather candidate replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba2fe521-bfcb-4ad4-9d35-b0fd3d8f9e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2497 3328\n"
     ]
    }
   ],
   "source": [
    "#  preliminaries\n",
    "sbd = [list(filter(lambda x: x.split('/')[1] not in {'PUNCT'}, v.strip('\\n').split(' '))) for v in open('./bitexts/seed.txt')]\n",
    "wct = Counter([w for l in open('./bitexts/seed.txt') for w in l.strip('\\n').split()])\n",
    "# gets lemma/pos pair counts\n",
    "#\n",
    "word2pos = defaultdict(lambda : Counter())\n",
    "for w,c in wct.items():\n",
    "    word2pos[w.split('/')[0].lower()][w.split('/')[1]] += c\n",
    "# frequency of PoS per lemma\n",
    "\n",
    "core_words = {w.split('/')[0].lower() for w,c in wct.items()\n",
    "              if word2pos[w.split('/')[0].lower()].most_common(1)[0][0] in core_pos}\n",
    "peri_words = {w.split('/')[0].lower() for w,c in wct.items()\n",
    "              if word2pos[w.split('/')[0].lower()].most_common(1)[0][0] in core_pos|peri_pos}\n",
    "# define core and peri(pheral) words on the basis of their dominant PoS\n",
    "print(len(core_words), len(peri_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae96217a-46d3-445d-8975-76a150463668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56613\n",
      "0 AVAANT 2025-04-19 12:01:44.838440\n",
      "25 CRHIBT 2025-04-19 12:01:53.264354\n",
      "50 LEFTBL 2025-04-19 12:02:03.087417\n",
      "75 NOAWBT 2025-04-19 12:02:12.777504\n",
      "100 YUZNTM 2025-04-19 12:02:22.825266\n",
      "125 TACPBC 2025-04-19 12:02:33.009422\n",
      "150 MLPTBL 2025-04-19 12:02:42.764288\n",
      "175 ROOWBT 2025-04-19 12:02:52.656363\n",
      "0 57245 (0, 1, 'book/NOUN') 2025-04-19 12:03:03.198994\n",
      "10000 57245 (1503, 15, 'tree/NOUN') 2025-04-19 12:03:07.392000\n",
      "20000 57245 (3077, 17, 'crowd/NOUN') 2025-04-19 12:03:11.193367\n",
      "30000 57245 (4684, 8, 'shut/VERB') 2025-04-19 12:03:14.063697\n",
      "40000 57245 (6361, 26, 'day/NOUN') 2025-04-19 12:03:17.248657\n",
      "50000 57245 (7770, 10, 'earth/NOUN') 2025-04-19 12:03:20.787949\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3342391/1997668771.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.log(fisher_exact([[a,b],[c,d]], alternative='greater')[1])\n",
      "/tmp/ipykernel_3342391/1997668771.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.log(fisher_exact([[a,b],[c,d]], alternative='greater')[1])\n",
      "/tmp/ipykernel_3342391/1997668771.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.log(fisher_exact([[a,b],[c,d]], alternative='greater')[1])\n",
      "/tmp/ipykernel_3342391/1997668771.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.log(fisher_exact([[a,b],[c,d]], alternative='greater')[1])\n",
      "/tmp/ipykernel_3342391/1997668771.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.log(fisher_exact([[a,b],[c,d]], alternative='greater')[1])\n",
      "/tmp/ipykernel_3342391/1997668771.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.log(fisher_exact([[a,b],[c,d]], alternative='greater')[1])\n",
      "/tmp/ipykernel_3342391/1997668771.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.log(fisher_exact([[a,b],[c,d]], alternative='greater')[1])\n",
      "/tmp/ipykernel_3342391/1997668771.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.log(fisher_exact([[a,b],[c,d]], alternative='greater')[1])\n",
      "/tmp/ipykernel_3342391/1997668771.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.log(fisher_exact([[a,b],[c,d]], alternative='greater')[1])\n",
      "/tmp/ipykernel_3342391/1997668771.py:4: RuntimeWarning: divide by zero encountered in log\n",
      "  return -np.log(fisher_exact([[a,b],[c,d]], alternative='greater')[1])\n"
     ]
    }
   ],
   "source": [
    "# run\n",
    "seed_index = get_seed_index(sbd, core_words)\n",
    "f2e, builder = get_counts_and_builder(doculects, core_words)\n",
    "bt_max = {doc : {wf : next((we for we,c in wes.most_common(1)),None)\n",
    "                 for wf,wes in bts.items()} for doc, bts in f2e.items()}\n",
    "# most frequent backtranslations per word type per language\n",
    "tok2bt, lemct = get_tok2bt(builder, bt_max)\n",
    "typ2bt, lgtot = get_typ2bt(tok2bt)\n",
    "lg2bt = get_lg2bt(typ2bt)\n",
    "candidates = get_candidates(typ2bt, lemct, lg2bt, lgtot, core_words, peri_words) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76304ec3-ffa8-43d8-b6ba-b15002beef36",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### iterative extraction of replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb24bd1-c922-4542-9b6f-64ad98507e06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top = max(candidates, key = lambda x : (len(candidates[x]), np.mean(list(candidates[x].values()))), default=None)\n",
    "# top contains on every iteration the candidate v_s,P pair with the greatest number of languages.\n",
    "replace_graph = nx.DiGraph()\n",
    "replacements = {}\n",
    "fh = open('./files/step1_output.txt','w')\n",
    "#\n",
    "while top != None and len(candidates[top]) >= MIN_D:\n",
    "    #\n",
    "    replace_graph, creates_cycle = creates_a_cycle(replace_graph, top) # avoids cycles in replacements\n",
    "    #\n",
    "    # Retrieve modeled tokens\n",
    "    tokens_per_lg = {lg : set() for lg in candidates[top]}\n",
    "    all_tokens_lem = set()\n",
    "    for t in filter(lambda t : t[2].split('/')[0].lower() == top[0], tok2bt):\n",
    "        all_tokens_lem.add(t)\n",
    "        for lg in filter(lambda lg : lg in tok2bt[t] and set(top[1]) <= tok2bt[t][lg], candidates[top]):\n",
    "            tokens_per_lg[lg].add(t)\n",
    "    tokens = set.union(*list(tokens_per_lg.values()) + [set()])\n",
    "    # tokens contains all modeled tokens given the split\n",
    "    #\n",
    "    # Writing output\n",
    "    avg_overlap = np.mean([len(vi&vj)/len(vi|vj) for vi,vj in combinations(tokens_per_lg.values(),2)])\n",
    "    print(top, len(candidates[top]), datetime.now())\n",
    "    fh.write('#%d %s %s\\tnLg=%d avgAssoc=%.2f coverage=%.2f (N=%d; overlap=%.2f)\\ncoverage per lg: %s\\n%s\\n\\n' % \n",
    "          (len(replacements), top[0],'+'.join(top[1]), len(candidates[top]),\n",
    "           np.mean(list(candidates[top].values())), len(tokens)/lemct[top[0]], len(tokens), avg_overlap,\n",
    "           Counter({lg:len(t) for lg,t in tokens_per_lg.items()}).most_common(), datetime.now()))\n",
    "    for lg in np.random.choice(list(candidates[top]), size=5):\n",
    "        W1,W2 = open('./bitexts/seed.txt').readlines(),open('./bitexts/%s.txt'% lg).readlines()     \n",
    "        for t in list(tokens_per_lg[lg])[:5]:\n",
    "            fh.write('>> covered    : %s %s\\t%s\\n%s%s\\n' % (lg, t, {w[1]:bt_max[lg][w[1]] for w in builder[t] if w[0] == lg}, W1[t[0]], W2[t[0]]))\n",
    "        for t in list(all_tokens_lem - tokens_per_lg[lg])[:5]:\n",
    "            fh.write('>> not covered: %s %s\\t%s\\n%s%s\\n' % (lg,t, {w[1]:bt_max[lg][w[1]] for w in builder[t] if w[0] == lg}, W1[t[0]], W2[t[0]]))\n",
    "    # If the additions does not contain a cycle, update the replacements dictionary\n",
    "    if creates_cycle: fh.write('==> creates a cycle\\n'); print('==> creates a cycle')\n",
    "    else: replacements[top] = tokens\n",
    "    #\n",
    "    # Part B: Update all counts\n",
    "    lemct[top[0]] -= len(tokens)\n",
    "    for m in tokens:\n",
    "        bts = tok2bt.pop(m)\n",
    "        for lg,bt in bts.items():\n",
    "            #if m[2].split('/')[0].lower() in bt: continue\n",
    "            lgtot[lg] -= 1\n",
    "            for i in range(len(bt)):\n",
    "                for bti in combinations(bt, i+1):\n",
    "                    #if m[2].split('/')[0].lower() in bt and m[2].split('/')[0].lower() not in bti: continue\n",
    "                    typ2bt[top[0]][lg][frozenset(bti)] -= 1\n",
    "                    lg2bt[lg][frozenset(bti)] -= 1\n",
    "    # Part C: Recalculate significance of v_s,P associations\n",
    "    for cand in list(filter(lambda c : c[0] == top[0], candidates)):\n",
    "        for lg in list(candidates[cand]):\n",
    "            a = typ2bt[top[0]][lg][top[1]]\n",
    "            b = lemct[top[0]] - a\n",
    "            c = (lg2bt[lg][top[1]] - a) if top[1] in lg2bt[lg] else 0\n",
    "            d = lgtot[lg] - (a+b+c)\n",
    "            score = -np.log(fisher_exact([[a,b],[c,d]], alternative='greater')[1])\n",
    "            if score <= -np.log(MIN_P): candidates[cand].pop(lg)\n",
    "            else: candidates[cand][lg] = score\n",
    "        if len(candidates[cand]) == 0: candidates.pop(cand)\n",
    "    top = max(candidates, key = lambda x : (len(candidates[x]), np.mean(list(candidates[x].values()))), default = None)\n",
    "fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b758cb-46ab-4cdd-8581-0820a7a24b5c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### write out replacement corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4af06447-48a0-4c5f-b1d3-08c8919611ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word2reps = {}\n",
    "for k in replacements:\n",
    "    word2reps[k[0]] = word2reps.get(k[0], []) + [k[1]]\n",
    "    \n",
    "lnew, dnew = [], []\n",
    "for li,l in enumerate(open('./bitexts/seed.dep')):\n",
    "    lnew.append([])\n",
    "    dnew.append([])\n",
    "    for wi,w in enumerate(l.strip('\\n').split()):\n",
    "        word,pos,ix,hix = w.split('/')\n",
    "        heads_deps = {wj.split('/')[0] for wj in l.strip('\\n').split() if wj != w and \n",
    "                      (wj.split('/')[2] == hix or wj.split('/')[3] == ix)}\n",
    "        #print(w, heads_deps) # block replacements if already a head or dep\n",
    "        if word in word2reps:\n",
    "            right_replacement = next((replacement for replacement in word2reps[word]\n",
    "                                      if (li,wi,word+'/'+pos) in replacements[word, replacement]), None)\n",
    "            if right_replacement != None:\n",
    "                replacement_l = [wj + '/' + word2pos[wj.lower()].most_common(1)[0][0] \n",
    "                                 for wj in right_replacement if wj not in heads_deps]\n",
    "            else:\n",
    "                replacement_l = [ word + '/' + pos ]\n",
    "        else:\n",
    "            replacement_l = [ word + '/' + pos ]\n",
    "        #\n",
    "        new_deps = list(map(lambda x : '%s/%s/%s' % (x,ix,hix), replacement_l))\n",
    "        # #print(new_deps)\n",
    "        lnew[-1].extend(replacement_l)\n",
    "        dnew[-1].extend(new_deps)\n",
    "    #print(dnew[-1])\n",
    "    \n",
    "with open('./bitexts/seed.splt', 'w') as fout:\n",
    "    fout.write('\\n'.join(' '.join(ln) for ln in lnew))\n",
    "with open('./bitexts/seed.splt.dep', 'w') as fout:\n",
    "    fout.write('\\n'.join(' '.join(ln) for ln in dnew))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6255bbd-44e3-4d7d-bc24-ace5fe05f09b",
   "metadata": {},
   "source": [
    "### generate Step-1 result table for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e4958227-c844-4f3a-8261-402fd34461eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{table}\n",
      "\\caption{Select output of Step 1 (top-20 extractions and every 30th extraction after)}\n",
      "\\begin{tabular}{rllrrrr}\n",
      "\\toprule\n",
      "rank & $v_s$ & $P$ & $N$ doculects & avg. -log $p$ & $N$ tokens & token coverage \\\\\n",
      "\\midrule\n",
      "1 & answer & answer+say & 94 & inf & 239 & 0.97 \\\\\n",
      "2 & write & write+say & 85 & 82.84 & 161 & 0.76 \\\\\n",
      "3 & scribe & law+scribe & 84 & 242.82 & 66 & 1.00 \\\\\n",
      "4 & widow & widow+woman & 77 & 57.86 & 27 & 0.96 \\\\\n",
      "5 & heal & heal+sick & 64 & 30.67 & 49 & 0.61 \\\\\n",
      "6 & vinegar & vinegar+wine & 64 & 30.24 & 6 & 1.00 \\\\\n",
      "7 & prostitute & prostitute+woman & 58 & 42.50 & 13 & 1.00 \\\\\n",
      "8 & forgive & sin+forgive & 54 & 47.37 & 61 & 0.94 \\\\\n",
      "9 & cup & cup+wine & 54 & 28.28 & 18 & 0.55 \\\\\n",
      "10 & hear & word+hear & 52 & 110.50 & 348 & 0.87 \\\\\n",
      "11 & knock & knock+door & 52 & 28.51 & 9 & 1.00 \\\\\n",
      "12 & come & to+come & 49 & 65.89 & 572 & 0.47 \\\\\n",
      "13 & life & life+eternal & 48 & 60.16 & 134 & 0.64 \\\\\n",
      "14 & repent & sin+repent & 47 & 57.79 & 56 & 0.98 \\\\\n",
      "15 & loaf & bread+loaf & 47 & 43.27 & 27 & 1.00 \\\\\n",
      "16 & leper & leper+sick & 47 & 42.47 & 13 & 1.00 \\\\\n",
      "17 & faith & faith+believe & 45 & 163.61 & 270 & 0.89 \\\\\n",
      "18 & cross & cross+crucify & 45 & 30.23 & 27 & 0.77 \\\\\n",
      "19 & centurion & soldier+centurion & 44 & 44.79 & 24 & 1.00 \\\\\n",
      "20 & synagogue & house+synagogue & 43 & 170.71 & 69 & 1.00 \\\\\n",
      "30 & bring & come+bring & 38 & 61.18 & 121 & 0.57 \\\\\n",
      "60 & prison & in+prison & 27 & 65.11 & 63 & 0.93 \\\\\n",
      "90 & wall & wall+city & 22 & 24.35 & 9 & 0.82 \\\\\n",
      "120 & clothing & garment+clothing & 18 & 22.82 & 25 & 0.83 \\\\\n",
      "150 & food & thing+eat & 15 & 65.67 & 41 & 0.91 \\\\\n",
      "180 & fever & body+fever & 13 & 31.08 & 8 & 1.00 \\\\\n",
      "210 & street & way+street & 12 & 22.42 & 13 & 1.00 \\\\\n",
      "240 & priest & priest+chief & 10 & 77.94 & 78 & 0.46 \\\\\n",
      "270 & finger & hand+finger & 9 & 32.71 & 7 & 0.88 \\\\\n",
      "300 & receive & receive+good & 8 & 33.90 & 43 & 0.16 \\\\\n",
      "330 & wild & in+wild & 7 & 73.48 & 31 & 0.74 \\\\\n",
      "360 & potter & potter+build & 7 & 19.22 & 3 & 1.00 \\\\\n",
      "390 & create & thing+create & 6 & 26.38 & 19 & 0.31 \\\\\n",
      "420 & sit & in+sit & 5 & 40.39 & 32 & 0.18 \\\\\n",
      "450 & skin & old+skin & 5 & 22.89 & 6 & 0.43 \\\\\n",
      "480 & shame & shame+to & 4 & 50.80 & 20 & 0.50 \\\\\n",
      "510 & jar & full+jar & 4 & 23.85 & 3 & 0.75 \\\\\n",
      "540 & sword & kill+sword & 4 & 19.53 & 6 & 0.17 \\\\\n",
      "570 & land & world+in & 3 & 46.13 & 25 & 0.41 \\\\\n",
      "600 & mute & not+speak & 3 & 27.26 & 9 & 0.64 \\\\\n",
      "630 & choke & choke+dead & 3 & 21.24 & 3 & 0.50 \\\\\n",
      "660 & stern & stern+ship & 3 & 18.95 & 2 & 0.50 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "step1_output = [[x.split(' ') for x in l.strip('\\n').split('\\t')] for l in open('./files/step1_output.txt')\n",
    "                if l[0] == '#' or l[:3] == '==>']\n",
    "step1_output = [s for si,s in enumerate(step1_output) if s[0][0] != '==>' and si < len(step1_output)-1 and step1_output[si+1][0][0] != '==>']\n",
    "df_builder = []\n",
    "for si,s in enumerate(step1_output):\n",
    "    if si < 20 or si % 30 == 29:\n",
    "        df_builder.append({'rank' : int(s[0][0][1:])+1, '$v_s$' : s[0][1], '$P$' : s[0][2], \n",
    "                           '$N$ doculects' : int(s[1][0].split('=')[1]),\n",
    "                           'avg. -log $p$' : float(s[1][1].split('=')[1]), \n",
    "                           '$N$ tokens' : int(s[1][3][3:-1]), \n",
    "                           'token coverage' : float(s[1][2].split('=')[1])})\n",
    "ctext = 'Select output of Step 1 (top-20 extractions and every 30th extraction after)'\n",
    "print(pd.DataFrame(df_builder).to_latex(index=False,caption=ctext,float_format='%.2f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be87824d-f28e-4b03-9da9-dbf72fa8527d",
   "metadata": {},
   "source": [
    "## Run eflomal on the circumlexified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1297d3-1990-42e4-bbc8-58bc9efe3338",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TARGTYPE = 'mrax'\n",
    "with Pool(12) as p:\n",
    "    p.starmap(run_eflomal, map(lambda doc : (doc, 'splt', TARGTYPE, './alignments/'), doculects))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57805275-abd9-4b78-ae36-0a0cbe0a96b3",
   "metadata": {},
   "source": [
    "## Step 2: gather synlexifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d99765d-8335-4ee1-afc1-e23d9563b8f6",
   "metadata": {},
   "source": [
    "### all potential synlexifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2258409d-9d12-452b-a03c-d7b46effd216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bad_k(k):\n",
    "    if k[0] == k[1]: return True\n",
    "    elif 'AFX' in k[0]+k[1]: return True\n",
    "    return False\n",
    "\n",
    "def word_type_check(k, core_words, peri_words):\n",
    "    return any(map(lambda wq : wq[0].lower() in core_words, k)) and all(map(lambda wq : wq[0].lower() in peri_words, k))  \n",
    "\n",
    "hits = defaultdict(lambda : set())\n",
    "ctr = 0 # scaffold\n",
    "for li,l in enumerate(open('./bitexts/seed.splt.dep').readlines()):\n",
    "    words = list(map(lambda w : w.split('/'), l.strip('\\n').split()))\n",
    "    ix2word = defaultdict(lambda : list())\n",
    "    word2align = {tuple(w) : i for i,w in enumerate(words)}\n",
    "    for word in words:\n",
    "        ix2word[word[2]] = ix2word.get(word[2], []) + [word]\n",
    "    #\n",
    "    for w in words:\n",
    "        if w[0].lower() in peri_words:\n",
    "            heads = ix2word[w[3]]\n",
    "            for head in heads:\n",
    "                if head == w: continue\n",
    "                wa,wb = sorted([tuple(w), tuple(head)])\n",
    "                k = (wa[:2], wb[:2])\n",
    "                if bad_k(k): \n",
    "                    continue\n",
    "                if word_type_check(k, core_words, peri_words):\n",
    "                    hits[k].add((li, word2align[wa], word2align[wb]))\n",
    "                if head[1] == 'ADP':\n",
    "                    headheads = ix2word[head[3]]\n",
    "                    for headhead in headheads:\n",
    "                        wa,wb = sorted([tuple(w), tuple(headhead)])\n",
    "                        k = (wa[:2], wb[:2])\n",
    "                        if bad_k(k):\n",
    "                            continue\n",
    "                        if word_type_check(k, core_words, peri_words):\n",
    "                            hits[k].add((li, word2align[wa], word2align[wb]))\n",
    "    for ix in ix2word:\n",
    "        for wx,wy in combinations(sorted(ix2word[ix]),2):\n",
    "            wa,wb = sorted([tuple(wx), tuple(wy)])\n",
    "            k = (wa[:2], wb[:2])\n",
    "            if word_type_check(k, core_words, peri_words):\n",
    "                hits[k].add((li, word2align[wa], word2align[wb]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "753a3e2f-37c3-425f-adce-e2556ce617c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "verse2syn = {}\n",
    "for h in hits:\n",
    "    if len(hits[h]) < 10: continue\n",
    "    for (v,wi,wj) in hits[h]:\n",
    "        verse2syn[v] = verse2syn.get(v, {}) | {(wi,wj) : h}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86445496-76d1-41b0-9700-7c2b2fcd3892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82559"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(v) for v in verse2syn.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff51e857-5869-4855-8d2b-e4ac10415cc7",
   "metadata": {},
   "source": [
    "### get candidate synlexifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5191c290-7a80-4947-b083-c995e0c61504",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_synlexifications(doc, verse2syn, verbose=False, outdir = './characterizations'):\n",
    "    eset = set()\n",
    "    ctt = Counter()\n",
    "    if not os.path.isdir(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    #\n",
    "    # read in\n",
    "    A = [[tuple(map(int, ax.split('-'))) for ax in a.strip('\\n').split()] \n",
    "         for a in open('./bitexts/%s.splt.sym' % (TARGTYPE, doc))]\n",
    "    L1 = [l.strip('\\n').split() for l in open('./bitexts/seed.splt')]\n",
    "    L2 = [l.strip('\\n').split() for l in open('./bitexts/%s.%s'% (doc,TARGTYPE)).readlines()]\n",
    "    #\n",
    "    # I: gather all\n",
    "    synlexifications = defaultdict(lambda : {})\n",
    "    for i,(a,w1,w2) in enumerate(zip(A, L1, L2)):\n",
    "        e2f_a, f2e_a = defaultdict(lambda : set()), defaultdict(lambda : set())\n",
    "        for ae,af in a: e2f_a[ae].add(af)\n",
    "        for ae,af in a: f2e_a[af].add(ae)\n",
    "        #\n",
    "        if i not in verse2syn: continue\n",
    "        for (wi,wj),h in verse2syn[i].items():\n",
    "            lab = '+'.join(sorted([w1[wi],w1[wj]]))\n",
    "            t1 = set(map(lambda x : w2[x], e2f_a[wi]))\n",
    "            t2 = set(map(lambda x : w2[x], e2f_a[wj]))\n",
    "            bt1 = {w1[x] for y in e2f_a[wi] for x in f2e_a[y] - {wi,wj}}\n",
    "            bt2 = {w1[x] for y in e2f_a[wi] for x in f2e_a[y] - {wi,wj}}\n",
    "            t1 = nobt1 = {w2[y] for y in e2f_a[wi] if f2e_a[y] - {wi,wj} == eset}\n",
    "            t2 = nobt2 = {w2[y] for y in e2f_a[wj] if f2e_a[y] - {wi,wj} == eset}\n",
    "            #\n",
    "            category = (4 if t1 & t2 != eset else \n",
    "                        (3 if t1 != eset and t2 != eset else\n",
    "                         (1 if t1 == eset and t2 != eset else (2 if t1 != eset and t2 == eset else 0))))\n",
    "            if verbose and TARGWRD in lab: \n",
    "                print(i, category, len(synlexifications[lab]), lab, t1, t2, bt1, bt2, '\\n' + ' '.join(w1) + '\\n' + ' '.join(w2) + '\\n')\n",
    "                ctt[tuple(t1),tuple(t2),category] += 1\n",
    "            synlexifications[lab][(i,(wi,wj))] = (tuple([tuple(t) for t in [t1,t2]]), category)\n",
    "    #\n",
    "    syn_builder = []\n",
    "    #\n",
    "    for i in sorted(verse2syn):\n",
    "        w1,w2 = L1[i], L2[i]\n",
    "        for (wi,wj), h in sorted(verse2syn[i].items()):\n",
    "            lab = '+'.join(sorted([w1[wi],w1[wj]]))\n",
    "            (t1,t2), cat = synlexifications[lab][i,(wi,wj)]\n",
    "            syn_builder.append({'category' : cat, 't1' : t1, 't2' : t2})\n",
    "    pd.DataFrame(syn_builder).to_excel('%s/%s.xlsx' % (outdir,doc))\n",
    "    if verbose: print(ctt.most_common(5))\n",
    "    print(doc, datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3566baac-dbb0-43a1-972d-48962e581caa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TARGTYPE = 'mrax'\n",
    "TARGWRD = 'enter/VERB+in/ADP'\n",
    "get_synlexifications('BOATBL', verse2syn, True, './results/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52f5c07-ad3f-4f60-bc41-e37165703a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outdir = './results/'\n",
    "pd.DataFrame([{'verse': v, 'wi':wi, 'wj':wj, 'class': '+'.join(['/'.join(hi) for hi in h])} \n",
    "             for v in sorted(verse2syn) for (wi,wj),h in sorted(verse2syn[v].items())]).to_excel(outdir + '/indices.xlsx')\n",
    "with Pool(4) as p:\n",
    "    p.starmap(get_synlexifications, ((doc, verse2syn, False, outdir) for doc in doculects))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b4b457-85d3-4249-8cb2-bb6a5313fdbe",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f96e601-abd5-4c96-9253-5235b7cce801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "evaluation = {\n",
    "    'wife.NOUN_woman.NOUN' : ['BOATBL','EUSNLT','MIQSBN'],\n",
    "    'clean.ADJ_un.AFX' : ['DJKWBT', 'JAVNRF','CRKWCV'],\n",
    "    'dead.ADJ_die.VERB' : ['ACMAS3','CHEIBT','INDASV'],\n",
    "    'king.NOUN_throne.NOUN': ['INDASV','KMSPNG'],\n",
    "    'whole.ADJ_world.NOUN': ['BVZYSS', 'DTSABM', 'INDASV'],\n",
    "    'go.VERB_way.NOUN' : ['AAUWBT', 'BOATBL', 'ITAR27','TPIPNG'],\n",
    "    'to.ADP_world.NOUN' : ['BOATBL', 'DJKWBT', 'EUSNLT'],\n",
    "    'from.ADP_go.VERB' : ['KHQBIV', 'ITAR27', 'KPVIBT'],\n",
    "    'go.VERB_out.ADP' : ['EUSNLT', 'ZNEZNE', 'INDASV'],\n",
    "    'go.VERB_up.ADP' : ['ROOWBT', 'KPVIBT', 'TIHBSM'],\n",
    "    'take.VERB_way.NOUN': ['DJKWBT', 'KGRLAI', 'KYCPNG', 'JAMBSW'],\n",
    "    'enter.VERB_in.ADP' : ['FUVLTBL', 'JAMBSW', 'YUJWBT'],\n",
    "    'door.NOUN_open.VERB' : ['KMOWBT', 'KBHWBT', 'CRNWBT', 'IBATIV'],\n",
    "    'answer.VERB_say.VERB' : ['TPIPNG', 'BIBWBT', 'IBATIV'],\n",
    "    'fish.NOUN_net.NOUN' : ['KYCPNG', 'MIQSBN' ],\n",
    "    'blind.ADJ_eye.NOUN' : ['KGRLAI', 'AAUWBT','CJPTJV','HAKTHV','MDYBSE','MZMWBT','NIJLAI','PRKBSM'],\n",
    "}\n",
    "print(len(evaluation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8aaa68e2-7d60-4024-a506-f025a909144e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82559\n"
     ]
    }
   ],
   "source": [
    "h2i = {}\n",
    "for h in sorted(verse2syn):\n",
    "    for t in sorted(verse2syn[h]):\n",
    "        h2i[h,t] = len(h2i)\n",
    "print(len(h2i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6e01d976-0184-4f37-af2c-05956da9e6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wife.NOUN_woman.NOUN (('wife', 'NOUN'), ('woman', 'NOUN')) 87\n",
      "clean.ADJ_un.AFX (('clean', 'ADJ'), ('un', 'AFX')) 46\n",
      "dead.ADJ_die.VERB (('dead', 'ADJ'), ('die', 'VERB')) 90\n",
      "king.NOUN_throne.NOUN (('king', 'NOUN'), ('throne', 'NOUN')) 62\n",
      "whole.ADJ_world.NOUN (('whole', 'ADJ'), ('world', 'NOUN')) 13\n",
      "go.VERB_way.NOUN (('go', 'VERB'), ('way', 'NOUN')) 112\n",
      "to.ADP_world.NOUN (('to', 'ADP'), ('world', 'NOUN')) 61\n",
      "from.ADP_go.VERB (('from', 'ADP'), ('go', 'VERB')) 109\n",
      "go.VERB_out.ADP (('go', 'VERB'), ('out', 'ADP')) 117\n",
      "go.VERB_up.ADP (('go', 'VERB'), ('up', 'ADP')) 61\n",
      "take.VERB_way.NOUN (('take', 'VERB'), ('way', 'NOUN')) 55\n",
      "enter.VERB_in.ADP (('enter', 'VERB'), ('in', 'ADP')) 191\n",
      "door.NOUN_open.VERB (('door', 'NOUN'), ('open', 'VERB')) 34\n",
      "answer.VERB_say.VERB (('answer', 'VERB'), ('say', 'VERB')) 279\n",
      "fish.NOUN_net.NOUN (('fish', 'NOUN'), ('net', 'NOUN')) 15\n",
      "blind.ADJ_eye.NOUN (('blind', 'ADJ'), ('eye', 'NOUN')) 58\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/files/blind.ADJ_eye.NOUN_KGRLAI.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m L1 \u001b[38;5;241m=\u001b[39m [l\u001b[38;5;241m.\u001b[39mstrip(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./bitexts/seed.splt\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m evaluation[k]:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/files/\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fout:\n\u001b[1;32m     10\u001b[0m         tot \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     11\u001b[0m         dfdoc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/files/\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m doc)\n",
      "File \u001b[0;32m~/yourvenvfoldername/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/files/blind.ADJ_eye.NOUN_KGRLAI.txt'"
     ]
    }
   ],
   "source": [
    "cats = ['unlexified', '1-underspecified', '2-underspecified', 'circumlexified', 'synlexified', 'infrequent/error']\n",
    "for k in evaluation:\n",
    "    kx = tuple(map(lambda x : tuple(x.split('.')), k.split('_')))\n",
    "    H = hits[kx]\n",
    "    print(k,kx,len(H))\n",
    "    if 'blind' not in k: continue\n",
    "    L1 = [l.strip('\\n').split() for l in open('./bitexts/seed.splt')]\n",
    "    for doc in evaluation[k]:\n",
    "        with open('/files/%s_%s.txt' % (k, doc), 'w') as fout:\n",
    "            tot = Counter()\n",
    "            dfdoc = pd.read_excel('/files/%s.xlsx' % doc)\n",
    "            L2 = [l.strip('\\n').split() for l in open('./bitexts/%s.%s'% (doc,TARGTYPE)).readlines()]\n",
    "            for i,(v,wi,wj) in enumerate(H):\n",
    "                try: fv = {c:dfdoc.iloc[h2i[v,(wi,wj)]][c] for c in ['category', 't1', 't2']}\n",
    "                except: continue\n",
    "                kk = str(fv['t1']),str(fv['t2']),str(fv['category']) \n",
    "                tot[kk] += 1\n",
    "                if i % len(H)//15 == 0:\n",
    "                    fout.write('verse = %s; words = (%d, %d)\\nextraction = %s\\n' % (v,wi,wj,fv))\n",
    "                    fout.write(' '.join(L1[v]) + '\\n' + ' '.join(L2[v]) + '\\n\\n')\n",
    "            fout.write('most common responses: %s\\n' % tot.most_common())\n",
    "            fout.write('most common categories: %s\\n' % \n",
    "                       {cats[int(c)]:v for c,v in Counter([c[2] for c,v in tot.items() for i in range(v)]).most_common()})\n",
    "            fout.write('-1 = infrequent/error; 0 = unlexified; 1 = 1-underspecified; 2 = 2-underspecified;\\n3 = circumlexified; 4 = synlexified')                 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
